{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport sys\nimport os\nimport math\nfrom PIL import Image \nimport spacy\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.utils.data as data\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport nltk\nfrom nltk.translate.bleu_score import corpus_bleu","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:32:15.504619Z","iopub.execute_input":"2022-04-23T12:32:15.504956Z","iopub.status.idle":"2022-04-23T12:32:26.187747Z","shell.execute_reply.started":"2022-04-23T12:32:15.504874Z","shell.execute_reply":"2022-04-23T12:32:26.187031Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\nspacy_eng = spacy.load('en_core_web_sm')","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:32:35.369564Z","iopub.execute_input":"2022-04-23T12:32:35.369826Z","iopub.status.idle":"2022-04-23T12:32:36.302820Z","shell.execute_reply.started":"2022-04-23T12:32:35.369797Z","shell.execute_reply":"2022-04-23T12:32:36.302114Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Vocabulary:\n    def __init__ (self, freq_threshold):\n        self.itos = {0:\"<PAD>\", 1:\"<SOS>\",2: \"<EOS>\", 3:\"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0,\"<SOS>\":1,\"<EOS>\":2,\"<UNK>\":3}\n        self.freq_threshold = freq_threshold\n    def __len__(self):\n        return len(self.itos)\n    @staticmethod\n    def tokenizer_eng(text):\n        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n    \n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = 4\n        for sentence in sentence_list:\n            for word in self.tokenizer_eng(sentence):\n                if word not in frequencies:\n                    frequencies[word] = 1\n                else:\n                    frequencies[word] += 1\n                    \n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer_eng(text)\n        return [self.stoi[token] if token in self.stoi else self.stoi['<UNK>'] for token in tokenized_text]","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:32:38.854838Z","iopub.execute_input":"2022-04-23T12:32:38.855236Z","iopub.status.idle":"2022-04-23T12:32:38.866369Z","shell.execute_reply.started":"2022-04-23T12:32:38.855199Z","shell.execute_reply":"2022-04-23T12:32:38.864738Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class CaptionDataset(Dataset):\n    def __init__(self,root_dir,captions_file,dataset_type,transform=None,freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(captions_file)\n        self.transform = transform\n        # Get img, caption columns\n        self.imgs_train = self.df[self.df['train']==True][\"Image_name\"]\n        self.captions_train = self.df[self.df['train']==True][\"Paragraph\"]\n    \n        if dataset_type == 'train':\n            self.imgs = self.df[self.df['train']==True][\"Image_name\"].tolist()\n            self.captions = self.df[self.df['train']==True][\"Paragraph\"].tolist()\n        elif dataset_type == 'validation':\n            self.imgs = self.df[self.df['val']==True][\"Image_name\"].tolist()\n            self.captions = self.df[self.df['val']==True][\"Paragraph\"].tolist()\n        elif dataset_type == 'test':\n            self.imgs = self.df[self.df['test']==True][\"Image_name\"].tolist()\n            self.captions = self.df[self.df['test']==True][\"Paragraph\"].tolist()\n        \n        # Initialize vocabulary and build vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocabulary(self.captions_train.tolist())\n        \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, index):\n        caption = self.captions[index]\n        img_id = self.imgs[index]\n        img_id = str(img_id)+'.jpg'\n        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n\n        if self.transform is not None:\n            img =  self.transform(img)\n\n        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n        numericalized_caption += self.vocab.numericalize(caption)\n        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n\n        return img, torch.tensor(numericalized_caption)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:32:41.613335Z","iopub.execute_input":"2022-04-23T12:32:41.614107Z","iopub.status.idle":"2022-04-23T12:32:41.633993Z","shell.execute_reply.started":"2022-04-23T12:32:41.614037Z","shell.execute_reply":"2022-04-23T12:32:41.632869Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class MyCollate:\n    def __init__ (self, pad_idx):\n        self.pad_idx = pad_idx\n        \n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n        return imgs, targets","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:32:44.861911Z","iopub.execute_input":"2022-04-23T12:32:44.862647Z","iopub.status.idle":"2022-04-23T12:32:44.869498Z","shell.execute_reply.started":"2022-04-23T12:32:44.862607Z","shell.execute_reply":"2022-04-23T12:32:44.868823Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_loader(root_folder,annotation_file,dataset_type,transform,freq_threshold=5,batch_size=32,num_workers=2,shuffle=True,pin_memory=True,):\n    dataset = CaptionDataset(root_folder, annotation_file, dataset_type,transform,freq_threshold)\n    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n    loader = DataLoader(dataset=dataset,\n                      batch_size=batch_size,\n                      num_workers=num_workers,\n                      shuffle=shuffle,\n                      pin_memory=pin_memory,\n                      collate_fn=MyCollate(pad_idx=pad_idx),\n                      )\n    return loader, dataset","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:32:47.576760Z","iopub.execute_input":"2022-04-23T12:32:47.577304Z","iopub.status.idle":"2022-04-23T12:32:47.582982Z","shell.execute_reply.started":"2022-04-23T12:32:47.577262Z","shell.execute_reply":"2022-04-23T12:32:47.582215Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Encoder inputs images and returns feature maps.\nclass EncoderCNN(nn.Module):\n    \"\"\"\n    Aruments:\n          - image - augmented image sample\n    Returns:\n           - features - feature maps of size (batch, height*width, #feature maps)\n    \"\"\"\n    def __init__(self):\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet152(pretrained=True)\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n        \n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n    def forward(self, images):\n        features = self.resnet(images)\n        # first, we need to resize the tensor to be \n        # (batch, size*size, feature_maps)\n        batch, feature_maps, size_1, size_2 = features.size()       \n        features = features.permute(0, 2, 3, 1)\n        features = features.view(batch, size_1*size_2, feature_maps)\n       \n        return features","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:32:50.215909Z","iopub.execute_input":"2022-04-23T12:32:50.216480Z","iopub.status.idle":"2022-04-23T12:32:50.224953Z","shell.execute_reply.started":"2022-04-23T12:32:50.216439Z","shell.execute_reply":"2022-04-23T12:32:50.223923Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Class performs Additive Bahdanau Attention\nclass BahdanauAttention(nn.Module):  \n    def __init__(self, num_features, hidden_dim, output_dim = 1):\n        super(BahdanauAttention, self).__init__()\n        self.num_features = num_features\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        # fully-connected layer to learn first weight matrix Wa\n        self.W_a = nn.Linear(self.num_features, self.hidden_dim)\n        # fully-connected layer to learn the second weight matrix Ua\n        self.U_a = nn.Linear(self.hidden_dim, self.hidden_dim)\n        # fully-connected layer to produce score (output), learning weight matrix va\n        self.v_a = nn.Linear(self.hidden_dim, self.output_dim)\n                \n    def forward(self, features, decoder_hidden):\n        \"\"\"\n        Arguments:\n            - features - features returned from Encoder\n            - decoder_hidden - hidden state output from Decoder\n                \n        Returns:\n            - context - context vector with a size of (1,2048)\n            - atten_weight - probabilities, express the feature relevance\n        \"\"\"\n        # add additional dimension to a hidden (required for summation)\n        decoder_hidden = decoder_hidden.unsqueeze(1)\n        atten_1 = self.W_a(features)\n        atten_2 = self.U_a(decoder_hidden)\n        # apply tangent to combine result from 2 fc layers\n        atten_tan = torch.tanh(atten_1+atten_2)\n        atten_score = self.v_a(atten_tan)\n        atten_weight = F.softmax(atten_score, dim = 1)\n        # first, we will multiply each vector by its softmax score\n        # next, we will sum up this vectors, producing the attention context vector\n        # the size of context equals to a number of feature maps\n        context = torch.sum(atten_weight * features,  dim = 1)\n        atten_weight = atten_weight.squeeze(dim=2)\n        \n        return context, atten_weight","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:32:53.336250Z","iopub.execute_input":"2022-04-23T12:32:53.337024Z","iopub.status.idle":"2022-04-23T12:32:53.346252Z","shell.execute_reply.started":"2022-04-23T12:32:53.336971Z","shell.execute_reply":"2022-04-23T12:32:53.345453Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n\n    \"\"\"Attributes:\n         - embedding_dim - specified size of embeddings;\n         - hidden_dim - the size of RNN layer (number of hidden states)\n         - vocab_size - size of vocabulary \n         - p - dropout probability\n    \"\"\"\n    def __init__(self, num_features, embedding_dim, hidden_dim, vocab_size, p =0.5):\n\n        super(DecoderRNN, self).__init__()\n        \n        self.num_features = num_features\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        # scale the inputs to softmax\n        self.sample_temp = 0.5 \n        \n        # embedding layer that turns words into a vector of a specified size\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        # LSTM will have a single layer of size 512 (512 hidden units)\n        # it will input concatinated context vector (produced by attention) \n        # and corresponding hidden state of Decoder\n        self.lstm = nn.LSTMCell(embedding_dim + num_features, hidden_dim)\n        # produce the final output\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        \n        # add attention layer \n        self.attention = BahdanauAttention(num_features, hidden_dim)\n        # dropout layer\n        self.drop = nn.Dropout(p=p)\n        # add initialization fully-connected layers\n        # initialize hidden state and cell memory using average feature vector \n        self.init_h = nn.Linear(num_features, hidden_dim)\n        self.init_c = nn.Linear(num_features, hidden_dim)\n\n    def forward(self, captions, features, sample_prob = 0.0):\n        \n        # create embeddings for captions of size (batch, sqe_len, embed_dim)\n        embed = self.embeddings(captions)\n        h, c = self.init_hidden(features)\n        seq_len = captions.size(1)\n        feature_size = features.size(1)\n        batch_size = features.size(0)\n        # these tensors will store the outputs from lstm cell and attention weights\n        outputs = torch.zeros(batch_size, seq_len, self.vocab_size).to(device)\n        atten_weights = torch.zeros(batch_size, seq_len, feature_size).to(device)\n\n        # scheduled sampling for training\n        # we do not use it at the first timestep (<start> word)\n        # but later we check if the probability is bigger than random\n        for t in range(seq_len):\n            sample_prob = 0.0 if t == 0 else 0.5\n            use_sampling = np.random.random() < sample_prob\n            if use_sampling == False:\n                word_embed = embed[:,t,:]\n            context, atten_weight = self.attention(features, h)\n            # input_concat shape at time step t = (batch, embedding_dim + hidden_dim)\n            input_concat = torch.cat([word_embed, context], 1)\n            h, c = self.lstm(input_concat, (h,c))\n            h = self.drop(h)\n            output = self.fc(h)\n            if use_sampling == True:\n                # use sampling temperature to amplify the values before applying softmax\n                scaled_output = output / self.sample_temp\n                scoring = F.log_softmax(scaled_output, dim=1)\n                top_idx = scoring.topk(1)[1]\n                word_embed = self.embeddings(top_idx).squeeze(1) \n            outputs[:, t, :] = output\n            atten_weights[:, t, :] = atten_weight\n        return outputs, atten_weights\n\n    def init_hidden(self, features):\n\n        \"\"\"Initializes hidden state and cell memory using average feature vector.\n        Arguments:\n            - features - features returned from Encoder\n    \n        Retruns:\n            - h0 - initial hidden state (short-term memory)\n            - c0 - initial cell state (long-term memory)\n        \"\"\"\n        mean_annotations = torch.mean(features, dim = 1)\n        h0 = self.init_h(mean_annotations)\n        c0 = self.init_c(mean_annotations)\n        return h0, c0\n    \n    def greedy_search(self, features, max_sentence = 30):\n\n        \"\"\"Greedy search to sample top candidate from distribution.\n        Arguments\n            - features - features returned from Encoder\n            - max_sentence - max number of token per caption (default=20)\n        Returns:\n            - sentence - list of tokens\n        \"\"\"\n\n        sentence = []\n        weights = []\n        input_word = torch.tensor(0).unsqueeze(0).to(device)\n        h, c = self.init_hidden(features)\n        while True:\n            embedded_word = self.embeddings(input_word)\n            context, atten_weight = self.attention(features, h)\n            # input_concat shape at time step t = (batch, embedding_dim + context size)\n            input_concat = torch.cat([embedded_word, context],  dim = 1)\n            h, c = self.lstm(input_concat, (h,c))\n            h = self.drop(h)\n            output = self.fc(h)\n            scoring = F.log_softmax(output, dim=1)\n            top_idx = scoring[0].topk(1)[1]\n            sentence.append(top_idx.item())\n            weights.append(atten_weight)\n            input_word = top_idx\n            if (len(sentence) >= max_sentence or top_idx == 1) or top_idx.item() == '<EOS>':\n                break\n        return sentence, weights\n","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:32:57.038427Z","iopub.execute_input":"2022-04-23T12:32:57.038805Z","iopub.status.idle":"2022-04-23T12:32:57.060662Z","shell.execute_reply.started":"2022-04-23T12:32:57.038768Z","shell.execute_reply":"2022-04-23T12:32:57.059862Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"transform_train = transforms.Compose([ \n    transforms.Resize(256),                          # smaller edge of image resized to 256\n    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n                         (0.229, 0.224, 0.225))])\n\n    \n# Setup the transforms\ntransform_test = transforms.Compose([ \n    transforms.Resize((224,224)),                    # smaller edge of image resized to 256\n    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n                         (0.229, 0.224, 0.225))])\n\n\ntrain_loader, dataset = get_loader(\n        '../input/stanford-image-paragraph-captioning-dataset/stanford_img/content/stanford_images',\n        '../input/stanford-image-paragraph-captioning-dataset/stanford_df_rectified.csv',\n        dataset_type='train',\n        transform = transform_train,\n        freq_threshold=6,batch_size=64)\n    \nvalid_loader, dataset = get_loader(\n        '../input/stanford-image-paragraph-captioning-dataset/stanford_img/content/stanford_images',\n        '../input/stanford-image-paragraph-captioning-dataset/stanford_df_rectified.csv',\n        dataset_type='validation',\n        transform = transform_test,\n        freq_threshold=6)   ","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:33:08.737719Z","iopub.execute_input":"2022-04-23T12:33:08.738013Z","iopub.status.idle":"2022-04-23T12:33:14.639107Z","shell.execute_reply.started":"2022-04-23T12:33:08.737980Z","shell.execute_reply":"2022-04-23T12:33:14.638303Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"embed_size = 256           # dimensionality of image and word embeddings\nhidden_size = 512          # number of features in hidden state of the RNN decoder\nnum_features = 2048        # number of feature maps, produced by Encoder\nvocab_size = vocab_size = len(dataset.vocab) # The size of the vocabulary.\n\n# Initialize the encoder and decoder. \nencoder = EncoderCNN()\ndecoder = DecoderRNN(num_features = num_features, \n                     embedding_dim = embed_size, \n                     hidden_dim = hidden_size, \n                     vocab_size = vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:33:20.767855Z","iopub.execute_input":"2022-04-23T12:33:20.768137Z","iopub.status.idle":"2022-04-23T12:33:27.143433Z","shell.execute_reply.started":"2022-04-23T12:33:20.768107Z","shell.execute_reply":"2022-04-23T12:33:27.142698Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/230M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"451b634fbe424e48b545d64bd7c212ab"}},"metadata":{}}]},{"cell_type":"code","source":"# Move models to GPU if CUDA is available. \ntorch.backends.cudnn.benchmark = True\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nencoder.to(device)\ndecoder.to(device)\n\n\n# Define the loss function. \ncriterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"]).cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n\n\n#params = list(decoder.parameters()) + list(encoder.parameters()) \nparams = list(decoder.parameters())\n\n# TODO #4: Define the optimizer.\noptimizer = torch.optim.Adam(params, lr = 1e-4)","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:33:32.446178Z","iopub.execute_input":"2022-04-23T12:33:32.446863Z","iopub.status.idle":"2022-04-23T12:33:39.666296Z","shell.execute_reply.started":"2022-04-23T12:33:32.446822Z","shell.execute_reply":"2022-04-23T12:33:39.665546Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"min_valid_loss = np.inf\nnum_epochs = 20\nfor ep in range(num_epochs):\n    train_loss = 0.0\n    encoder.eval() # no fine-tuning for Encoder\n    decoder.train()\n    for images, captions in train_loader:\n        captions = captions.transpose(0,1)\n        captions_target = captions[:, 1:].to(device) \n        # captions for training without the last word\n        captions_train = captions[:, :-1].to(device)\n        \n        # Move batch of images and captions to GPU if CUDA is available.\n        images = images.to(device)\n        # Zero the gradients.\n        decoder.zero_grad()\n        encoder.zero_grad()\n        \n        # Pass the inputs through the CNN-RNN model.\n        features = encoder(images)\n        outputs, atten_weights = decoder(captions= captions_train,features = features)\n        \n        # Calculate the batch loss.\n        loss = criterion(outputs.view(-1, vocab_size), captions_target.reshape(-1)) \n        #loss = criterion(outputs.reshape(-1, outputs.shape[2]),captions.reshape(-1)) \n\n        loss.backward(loss)\n        optimizer.step()\n\n        train_loss += loss.item()\n            \n    valid_loss = 0.0\n    # evaluation of encoder and decoder\n    encoder.eval()\n    decoder.eval()    \n    for images, captions in valid_loader:\n        captions = captions.transpose(0,1)\n        captions_target = captions[:, 1:].to(device) \n        captions = captions[:, :-1].to(device)\n        images = images.to(device)\n        features = encoder(images)\n        outputs, atten_weights = decoder(captions= captions,features = features)\n        loss = criterion(outputs.view(-1, vocab_size),captions_target.reshape(-1))\n        valid_loss += loss.item()\n            \n    avg_train_loss = train_loss / len(train_loader)\n    avg_valid_loss = valid_loss / len(valid_loader)\n    print(f'Epoch {ep+1} \\t\\t Training Loss: {avg_train_loss} \\t\\t Validation Loss: {avg_valid_loss}')\n    if min_valid_loss > avg_valid_loss:\n        print(f'Validation Loss Decreased({min_valid_loss:.6f} to {avg_valid_loss:.6f}) \\t Saving The Model')\n        min_valid_loss = avg_valid_loss\n        # Saving State Dict\n        torch.save(encoder.state_dict(), '/kaggle/working/encoder_attention_model.pth')\n        torch.save(decoder.state_dict(), '/kaggle/working/decoder_attention_model.pth')\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-23T12:33:56.360977Z","iopub.execute_input":"2022-04-23T12:33:56.361555Z","iopub.status.idle":"2022-04-23T14:01:28.813640Z","shell.execute_reply.started":"2022-04-23T12:33:56.361516Z","shell.execute_reply":"2022-04-23T14:01:28.812614Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1 \t\t Training Loss: 5.5506329787404916 \t\t Validation Loss: 5.042238235473633\nValidation Loss Decreased(inf to 5.042238) \t Saving The Model\nEpoch 2 \t\t Training Loss: 5.04537540360501 \t\t Validation Loss: 4.837048885149834\nValidation Loss Decreased(5.042238 to 4.837049) \t Saving The Model\nEpoch 3 \t\t Training Loss: 4.863338733974256 \t\t Validation Loss: 4.686374572607187\nValidation Loss Decreased(4.837049 to 4.686375) \t Saving The Model\nEpoch 4 \t\t Training Loss: 4.738284460285254 \t\t Validation Loss: 4.582415868074466\nValidation Loss Decreased(4.686375 to 4.582416) \t Saving The Model\nEpoch 5 \t\t Training Loss: 4.639540498716789 \t\t Validation Loss: 4.504520520185813\nValidation Loss Decreased(4.582416 to 4.504521) \t Saving The Model\nEpoch 6 \t\t Training Loss: 4.566060967612684 \t\t Validation Loss: 4.425491094589233\nValidation Loss Decreased(4.504521 to 4.425491) \t Saving The Model\nEpoch 7 \t\t Training Loss: 4.50542534234231 \t\t Validation Loss: 4.375773778328528\nValidation Loss Decreased(4.425491 to 4.375774) \t Saving The Model\nEpoch 8 \t\t Training Loss: 4.458634669320626 \t\t Validation Loss: 4.339066236447065\nValidation Loss Decreased(4.375774 to 4.339066) \t Saving The Model\nEpoch 9 \t\t Training Loss: 4.405476001271031 \t\t Validation Loss: 4.287237399663681\nValidation Loss Decreased(4.339066 to 4.287237) \t Saving The Model\nEpoch 10 \t\t Training Loss: 4.367236388357062 \t\t Validation Loss: 4.265489816665649\nValidation Loss Decreased(4.287237 to 4.265490) \t Saving The Model\nEpoch 11 \t\t Training Loss: 4.344361972390559 \t\t Validation Loss: 4.231796934054448\nValidation Loss Decreased(4.265490 to 4.231797) \t Saving The Model\nEpoch 12 \t\t Training Loss: 4.31257091384185 \t\t Validation Loss: 4.200948327015608\nValidation Loss Decreased(4.231797 to 4.200948) \t Saving The Model\nEpoch 13 \t\t Training Loss: 4.274033426192769 \t\t Validation Loss: 4.174580607658777\nValidation Loss Decreased(4.200948 to 4.174581) \t Saving The Model\nEpoch 14 \t\t Training Loss: 4.255121877318935 \t\t Validation Loss: 4.154685760155703\nValidation Loss Decreased(4.174581 to 4.154686) \t Saving The Model\nEpoch 15 \t\t Training Loss: 4.232330798057088 \t\t Validation Loss: 4.123593804163811\nValidation Loss Decreased(4.154686 to 4.123594) \t Saving The Model\nEpoch 16 \t\t Training Loss: 4.223333712209735 \t\t Validation Loss: 4.122969645720262\nValidation Loss Decreased(4.123594 to 4.122970) \t Saving The Model\nEpoch 17 \t\t Training Loss: 4.194276412328084 \t\t Validation Loss: 4.087046439831074\nValidation Loss Decreased(4.122970 to 4.087046) \t Saving The Model\nEpoch 18 \t\t Training Loss: 4.169181497473466 \t\t Validation Loss: 4.06364243152814\nValidation Loss Decreased(4.087046 to 4.063642) \t Saving The Model\nEpoch 19 \t\t Training Loss: 4.157607109923112 \t\t Validation Loss: 4.078721682230632\nEpoch 20 \t\t Training Loss: 4.1399824901631 \t\t Validation Loss: 4.038688622988188\nValidation Loss Decreased(4.063642 to 4.038689) \t Saving The Model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**INFERENCE**","metadata":{}},{"cell_type":"code","source":"transform_test = transforms.Compose([ \n    transforms.Resize((224,224)),                    # smaller edge of image resized to 256\n    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n                         (0.229, 0.224, 0.225))])\n\n\ntest_loader, dataset = get_loader(\n        '../input/stanford-image-paragraph-captioning-dataset/stanford_img/content/stanford_images',\n        '../input/stanford-image-paragraph-captioning-dataset/stanford_df_rectified.csv',\n        dataset_type='test',\n        transform = transform_test,\n        freq_threshold=6)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:09:22.052038Z","iopub.execute_input":"2022-04-22T17:09:22.052756Z","iopub.status.idle":"2022-04-22T17:09:25.514185Z","shell.execute_reply.started":"2022-04-22T17:09:22.052719Z","shell.execute_reply":"2022-04-22T17:09:25.513438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/stanford-image-paragraph-captioning-dataset/stanford_df_rectified.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:09:30.938711Z","iopub.execute_input":"2022-04-22T17:09:30.938992Z","iopub.status.idle":"2022-04-22T17:09:31.048724Z","shell.execute_reply.started":"2022-04-22T17:09:30.938965Z","shell.execute_reply":"2022-04-22T17:09:31.047933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = df[df['train']==True]\nvocabulary = Vocabulary(6)\ncaptions = train[\"Paragraph\"]\nvocabulary.build_vocabulary(captions.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:09:35.135713Z","iopub.execute_input":"2022-04-22T17:09:35.136182Z","iopub.status.idle":"2022-04-22T17:09:37.582373Z","shell.execute_reply.started":"2022-04-22T17:09:35.13615Z","shell.execute_reply":"2022-04-22T17:09:37.581621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_size = 256           # dimensionality of image and word embeddings\nhidden_size = 512          # number of features in hidden state of the RNN decoder\nnum_features = 2048        # number of feature maps, produced by Encoder\nvocab_size = vocab_size = len(dataset.vocab) # The size of the vocabulary.\n\nencoder = EncoderCNN()\ndecoder = DecoderRNN(num_features = num_features, \n                     embedding_dim = embed_size, \n                     hidden_dim = hidden_size, \n                     vocab_size = vocab_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:09:41.737309Z","iopub.execute_input":"2022-04-22T17:09:41.737564Z","iopub.status.idle":"2022-04-22T17:09:51.927077Z","shell.execute_reply.started":"2022-04-22T17:09:41.737536Z","shell.execute_reply":"2022-04-22T17:09:51.926319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the trained weights.\nencoder.load_state_dict(torch.load('../input/attention-model/encoder_attention_model.pth'))\ndecoder.load_state_dict(torch.load('../input/attention-model/decoder_attention_model.pth'))\n# Move models to GPU if CUDA is available.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nencoder.to(device)\ndecoder.to(device)\ndecoder.eval()\nencoder.eval()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:09:55.034202Z","iopub.execute_input":"2022-04-22T17:09:55.034513Z","iopub.status.idle":"2022-04-22T17:10:05.094581Z","shell.execute_reply.started":"2022-04-22T17:09:55.034481Z","shell.execute_reply":"2022-04-22T17:10:05.093919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_sentence(output):\n    vocab = vocabulary.itos\n    words = [vocab.get(idx) for idx in output]\n    #words = [word for word in words if word not in ('<start>','<end>')]\n    sentence = \" \".join(words)\n    \n    return sentence\n\ndef get_prediction(image):\n    plt.imshow(image)\n    plt.title('Sample Image')\n    plt.show()\n    image =  transform_test(image).unsqueeze(0).to(device)\n    features = encoder(image)\n    output, atten_weights = decoder.greedy_search(features)    \n    sentence = clean_sentence(output)\n    print(sentence)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:10:17.074095Z","iopub.execute_input":"2022-04-22T17:10:17.07443Z","iopub.status.idle":"2022-04-22T17:10:17.081416Z","shell.execute_reply.started":"2022-04-22T17:10:17.074391Z","shell.execute_reply":"2022-04-22T17:10:17.080334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nsmoothie = SmoothingFunction().method4\n\ndef calculate_blue(y_pred, y_true):\n  actual, predicted = list(), list()\n  for i in range(len(y_pred)):\n    predicted.append(y_pred[i].split())\n    actual.append(y_true[i].split())\n  # calculate BLEU score\n  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0),smoothing_function=smoothie))\n  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie))\n  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie))\n  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie))","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:20:23.623156Z","iopub.execute_input":"2022-04-22T17:20:23.623429Z","iopub.status.idle":"2022-04-22T17:20:23.631589Z","shell.execute_reply.started":"2022-04-22T17:20:23.623403Z","shell.execute_reply":"2022-04-22T17:20:23.630916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = df[df['test']==True]\ntest_images = test.Image_name.to_list()\ny_pred = list()\ny_true = list()\nimage_path = '../input/stanford-image-paragraph-captioning-dataset/stanford_img/content/stanford_images/'\nfor name in test_images[:200]:\n  img = Image.open(image_path+str(name)+'.jpg').convert(\"RGB\")\n  image =  transform_test(img).unsqueeze(0).to(device)\n  features = encoder(image)\n  output, atten_weights = decoder.greedy_search(features)    \n  y_hat = clean_sentence(output)\n  y_pred.append(y_hat)\n  y_true.append(test[test.Image_name == name].Paragraph.values[0])\n\nprint('BLUE Scores for test data\\n'+'-'*25)\ncalculate_blue(y_pred, y_true)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:20:32.706903Z","iopub.execute_input":"2022-04-22T17:20:32.707173Z","iopub.status.idle":"2022-04-22T17:20:44.437599Z","shell.execute_reply.started":"2022-04-22T17:20:32.707145Z","shell.execute_reply":"2022-04-22T17:20:44.436784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open(image_path+str(2414610)+'.jpg').convert(\"RGB\")\nimage =  transform_test(img).unsqueeze(0).to(device)\nfeatures = encoder(image)\noutput, atten_weights = decoder.greedy_search(features,max_sentence = 50)    \ncaption = clean_sentence(output)\ncaption","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:17:05.207801Z","iopub.execute_input":"2022-04-22T17:17:05.20809Z","iopub.status.idle":"2022-04-22T17:17:05.277557Z","shell.execute_reply.started":"2022-04-22T17:17:05.208048Z","shell.execute_reply":"2022-04-22T17:17:05.276874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generating Captions**","metadata":{}},{"cell_type":"code","source":"num = 5\ncount = 1\n\nfig = plt.figure(figsize=(10,20))\nfor name in test_images[0:5]:\n   img = Image.open(image_path+str(name)+'.jpg').convert(\"RGB\").resize((224,224))\n   image =  transform_test(img).unsqueeze(0).to(device)\n   features = encoder(image)\n   output, atten_weights = decoder.greedy_search(features,max_sentence = 30)    \n   paragraph = clean_sentence(output)\n   ax = fig.add_subplot(num,2,count,xticks=[],yticks=[])\n   ax.imshow(img)\n   count += 1\n\n   captions = paragraph.split('.')\n   captions = [caption.strip() for caption in captions if len(caption)>1]\n\n   ax = fig.add_subplot(num,2,count)\n   plt.axis('off')\n   ax.plot()\n   ax.set_xlim(0,1)\n   ax.set_ylim(0,len(captions))\n   for i, caption in enumerate(captions):\n     ax.text(0,i,caption,fontsize=20)\n   count += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T06:05:47.956445Z","iopub.execute_input":"2022-04-22T06:05:47.956738Z","iopub.status.idle":"2022-04-22T06:05:49.009673Z","shell.execute_reply.started":"2022-04-22T06:05:47.956708Z","shell.execute_reply":"2022-04-22T06:05:49.008867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open('../input/stanford-image-paragraph-captioning-dataset/stanford_img/content/stanford_images/2414610.jpg').convert(\"RGB\")\nimage =  transform_test(img).unsqueeze(0).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T12:23:28.235285Z","iopub.execute_input":"2022-04-20T12:23:28.235591Z","iopub.status.idle":"2022-04-20T12:23:28.259538Z","shell.execute_reply.started":"2022-04-20T12:23:28.235559Z","shell.execute_reply":"2022-04-20T12:23:28.258522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Move image Pytorch Tensor to GPU if CUDA is available.\nimage = image.to(device)\n\n# Obtain the embedded image features.\nprint(image.shape)\nfeatures = encoder(image)\n\n# Pass the embedded image features through the model to get a predicted caption.\noutput, atten_weights = decoder.greedy_search(features)\nprint('example output:', output)\n\nassert (type(output)==list), \"Output needs to be a Python list\" \nassert all([type(x)==int for x in output]), \"Output should be a list of integers.\" \nassert all([x in vocabulary.itos for x in output]), \"Each entry in the output needs to correspond to an integer that indicates a the words in vocabulary\"","metadata":{"execution":{"iopub.status.busy":"2022-04-20T12:23:32.622015Z","iopub.execute_input":"2022-04-20T12:23:32.62228Z","iopub.status.idle":"2022-04-20T12:23:32.687218Z","shell.execute_reply.started":"2022-04-20T12:23:32.622252Z","shell.execute_reply":"2022-04-20T12:23:32.686431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing Attention**","metadata":{}},{"cell_type":"code","source":"image = Image.open('../input/stanford-image-paragraph-captioning-dataset/stanford_img/content/stanford_images/2414610.jpg').convert(\"RGB\")\nget_prediction(image)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T06:08:16.990718Z","iopub.execute_input":"2022-04-22T06:08:16.991023Z","iopub.status.idle":"2022-04-22T06:08:17.292884Z","shell.execute_reply.started":"2022-04-22T06:08:16.990993Z","shell.execute_reply":"2022-04-22T06:08:17.291672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for plot attention in the image sample\ndef visualize_attention(image, words, atten_weights):\n    fig = plt.figure(figsize=(14,12)) \n    len_tokens = len(words)\n    \n    for i in range(len(words)):\n        atten_current = atten_weights[i].detach().cpu().numpy()\n        atten_current = atten_current.reshape(7,7)       \n        ax = fig.add_subplot(len_tokens//3, len_tokens//3, i+1)\n        ax.set_title(words[i])\n        img = ax.imshow(np.squeeze(image))\n        ax.imshow(atten_current, cmap='gray', alpha=0.8, extent=img.get_extent(), interpolation = 'bicubic')\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T06:08:50.253245Z","iopub.execute_input":"2022-04-22T06:08:50.253656Z","iopub.status.idle":"2022-04-22T06:08:50.261494Z","shell.execute_reply.started":"2022-04-22T06:08:50.253621Z","shell.execute_reply":"2022-04-22T06:08:50.260582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = Image.open('../input/stanford-image-paragraph-captioning-dataset/stanford_img/content/stanford_images/2414610.jpg').convert(\"RGB\")\nimage =  transform_test(img).unsqueeze(0).to(device)\n# Move image Pytorch Tensor to GPU if CUDA is available.\nimage = image.to(device)\n\n# Obtain the embedded image features.\nfeatures = encoder(image)\n\n# Pass the embedded image features through the model to get a predicted caption.\noutput, atten_weights = decoder.greedy_search(features)\n\nvocab = vocabulary.itos\nwords = [vocab.get(idx) for idx in output]\nwords = [word for word in words if word not in ('<start>','<end>')]\n\n\nvisualize_attention(img, words, atten_weights)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T06:09:34.469413Z","iopub.execute_input":"2022-04-22T06:09:34.469979Z","iopub.status.idle":"2022-04-22T06:09:37.980956Z","shell.execute_reply.started":"2022-04-22T06:09:34.469928Z","shell.execute_reply":"2022-04-22T06:09:37.980071Z"},"trusted":true},"execution_count":null,"outputs":[]}]}